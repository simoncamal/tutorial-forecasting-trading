{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simoncamal/tutorial-forecasting-trading/blob/main/previ_extremes_gefcom.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraries\n"
      ],
      "metadata": {
        "id": "eOhNUAYgLwHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import plotly.express as px\n",
        "!pip install gurobipy\n",
        "import gurobipy as gp\n",
        "from gurobipy import GRB\n",
        "from scipy import signal\n",
        "from sklearn.linear_model import LassoCV, Lasso"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCHMdq6rLzIZ",
        "outputId": "15e4c870-2168-47c4-8067-c399a69f76ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gurobipy\n",
            "  Downloading gurobipy-10.0.3-cp310-cp310-manylinux2014_x86_64.whl (12.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gurobipy\n",
            "Successfully installed gurobipy-10.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!wget https://raw.githubusercontent.com/simoncamal/tutorial-forecasting-trading/main/tutorial_data/dataset_gefcom_prod_logit.pkl\n",
        "!wget https://raw.githubusercontent.com/simoncamal/tutorial-forecasting-trading/main/tutorial_data/dataset_gefcom_wd.pkl\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MA8D9SKaNQit",
        "outputId": "f7fbbde1-bdb0-4426-9158-0ea958b46f58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-25 14:07:30--  https://raw.githubusercontent.com/simoncamal/tutorial-forecasting-trading/main/tutorial_data/dataset_gefcom_prod_logit.pkl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1534698 (1.5M) [application/octet-stream]\n",
            "Saving to: ‘dataset_gefcom_prod_logit.pkl’\n",
            "\n",
            "dataset_gefcom_prod 100%[===================>]   1.46M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-11-25 14:07:31 (34.5 MB/s) - ‘dataset_gefcom_prod_logit.pkl’ saved [1534698/1534698]\n",
            "\n",
            "--2023-11-25 14:07:31--  https://raw.githubusercontent.com/simoncamal/tutorial-forecasting-trading/main/tutorial_data/dataset_gefcom_wd.pkl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1544970 (1.5M) [application/octet-stream]\n",
            "Saving to: ‘dataset_gefcom_wd.pkl’\n",
            "\n",
            "dataset_gefcom_wd.p 100%[===================>]   1.47M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-11-25 14:07:31 (39.7 MB/s) - ‘dataset_gefcom_wd.pkl’ saved [1544970/1544970]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definition training testing\n",
        "\n"
      ],
      "metadata": {
        "id": "2j0GR5DeLFUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_prod = pd.read_pickle('/content/dataset_gefcom_prod_logit.pkl')\n",
        "dataset_prod = dataset_prod.set_index('TIMESTAMP')\n",
        "dataset_wd = pd.read_pickle('/content/dataset_gefcom_wd.pkl')\n",
        "\n",
        "conditioning_number_points = 10\n",
        "conditioning_winddirection_points_positions = np.arange(0, 360, 36)\n",
        "conditioning_winddirection_bandwidths = [90]\n",
        "number_plants = 10\n",
        "number_horizons = 6\n",
        "lags = [0, 1, 2, 4, 6]\n",
        "alphas_lasso = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08]\n",
        "\n",
        "target_site = dataset_wd.columns[0]\n",
        "testing_month_index = 1\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0e80MoczvvQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Aux functions: dataset preprocessing"
      ],
      "metadata": {
        "id": "klF48gOU52ZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test_splits_windDirection(dataset_wd, testing_month_index):\n",
        "  wd_train_base = dataset_wd[dataset_wd.index.year == 2012]\n",
        "  wd_test_month = dataset_wd[(dataset_wd.index.year == 2013)&(dataset_wd.index.month == testing_month_index)]\n",
        "  wd_train = dataset_wd[dataset_wd.index < wd_test_month.index[0]]\n",
        "\n",
        "  return wd_train, wd_test_month\n",
        "\n",
        "\n",
        "def train_test_splits_Production(dataset_prod, testing_month_index):\n",
        "  prod_train_base = dataset_prod[dataset_prod.index.year == 2012]\n",
        "  prod_test_month = dataset_prod[(dataset_prod.index.year == 2013)&(dataset_prod.index.month == testing_month_index)]\n",
        "  prod_train = dataset_prod[dataset_prod.index < prod_test_month.index[0]]\n",
        "\n",
        "  return prod_train, prod_test_month"
      ],
      "metadata": {
        "id": "eRG4luHl57oZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "cor prod"
      ],
      "metadata": {
        "id": "OpveCeKYEEiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prod_train = dataset_prod[dataset_prod.index.year == 2012]\n",
        "cor_prod = []\n",
        "for target_site in np.arange(1, 11, 1):\n",
        "  cor_prod_target = pd.DataFrame({'cor':\n",
        "    np.asarray([np.correlate(prod_train[str(target_site)].values,\n",
        "                            prod_train[col].values)/(np.std(prod_train[str(target_site)].values)*np.std(prod_train[col].values)*prod_train.shape[0])\n",
        "                            for col in prod_train.columns]).flatten(),\n",
        "                                  'site_i': target_site, 'site_j':prod_train.columns})\n",
        "  cor_prod.append(cor_prod_target)\n",
        "cor_prod = pd.concat(cor_prod)\n",
        "print(cor_prod)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbCwiSJGEFui",
        "outputId": "6f2bbf75-9a09-46b8-b5b2-30f2795deaa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         cor  site_i site_j\n",
            "0   1.412208       1      1\n",
            "1   0.816240       1      2\n",
            "2   0.651848       1      3\n",
            "3   0.722273       1      4\n",
            "4   0.562570       1      5\n",
            "..       ...     ...    ...\n",
            "5   0.697741      10      6\n",
            "6   0.427707      10      7\n",
            "7   0.395371      10      8\n",
            "8   0.472078      10      9\n",
            "9   1.042543      10     10\n",
            "\n",
            "[100 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ccf wd"
      ],
      "metadata": {
        "id": "vIh8CsilhtYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ccf_target_train = np.asarray([np.correlate(wd_train[target_site].values, wd_train[col].values)/(np.std(wd_train[target_site].values)*np.std(wd_train[col].values)*wd_train.shape[0]) for col in wd_train.columns]).flatten()\n",
        "ccf_target_test = pd.DataFrame({'ccf_wd':\n",
        "  np.asarray([np.correlate(wd_test_month[target_site].values, wd_test_month[col].values)/(np.std(wd_test_month[target_site].values)*np.std(wd_test_month[col].values)*wd_test_month.shape[0]) for col in wd_test_month.columns]).flatten(),\n",
        "                                'site_i': target_site, 'site_j':wd_train.columns})\n",
        "print(ccf_target_test.sort_values(by='ccf_wd', ascending=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "fjXdRcpjhwTc",
        "outputId": "b4211bed-1b04-413e-aa40-923c4cbc10cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-3b245933d0d1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mccf_target_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrelate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwd_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_site\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwd_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_site\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwd_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwd_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwd_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m ccf_target_test = pd.DataFrame({'ccf_wd':\n\u001b[1;32m      3\u001b[0m   \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrelate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwd_test_month\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_site\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd_test_month\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwd_test_month\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_site\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwd_test_month\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwd_test_month\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwd_test_month\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                 'site_i': target_site, 'site_j':wd_train.columns})\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mccf_target_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ccf_wd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'wd_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lasso conditioned by wind direction\n",
        "\n",
        "\n",
        "def buildExplanVar(dataset, lags, horizon, number_plants, explanatory_sites):\n",
        "    explan_vars = dataset[[str(s) for s in explanatory_sites]]\n",
        "    explan_vars_lagged = explan_vars.assign(\n",
        "        **{f'{col} (t-{lag + horizon})': explan_vars[col].shift(lag + horizon).fillna(method='bfill')\n",
        "           for lag in lags for col in explan_vars})\n",
        "    explan_vars_lagged = explan_vars_lagged.iloc[:, number_plants:]  # exclude observed at time t\n",
        "    explan_vars_lagged = explan_vars_lagged.assign(intercept=1)\n",
        "\n",
        "    return explan_vars_lagged\n",
        "\n",
        "\n",
        "def buildARST(dataset, explanatory_sites,\n",
        "              nb_plants, lags, horizon, target_site, plot=False):\n",
        "\n",
        "    explan_vars_lagged = buildExplanVar(dataset=dataset, explanatory_sites=explanatory_sites, lags=lags,\n",
        "                                        horizon=horizon,\n",
        "                                        number_plants=nb_plants)\n",
        "    explan_vars_lagged_mat = explan_vars_lagged.values\n",
        "    explan_vars_lagged_mat_t = np.transpose(explan_vars_lagged_mat)\n",
        "    explan_covariance_matrix = np.matmul(explan_vars_lagged_mat_t, explan_vars_lagged_mat)\n",
        "    norm_factor = np.max(explan_covariance_matrix)\n",
        "    explan_covariance_matrix = explan_covariance_matrix / norm_factor\n",
        "\n",
        "    explan_precision_matrix = linalg.inv(explan_covariance_matrix)\n",
        "    xty_matrix = np.matmul(explan_vars_lagged_mat_t, dataset[str(target_site)].values) / norm_factor\n",
        "    response = dataset[str(target_site)].values / norm_factor\n",
        "    beta_ar_noweights = np.matmul(explan_precision_matrix, xty_matrix)\n",
        "    beta_ar_noweights_df = pd.DataFrame({'beta': beta_ar_noweights,\n",
        "                                         'explanatory_site': np.append(\n",
        "                                             np.concatenate([np.repeat(site, len(lags)) for site in explanatory_sites]),\n",
        "                                             'intercept'),\n",
        "                                         'explanatory_lag': np.append(\n",
        "                                             np.concatenate([[lag + horizon for lag in lags] for i in range(nb_plants)]),\n",
        "                                             'NA')})\n",
        "    beta_ar_noweights_wide_df = beta_ar_noweights_df.pivot_table(values='beta', columns='explanatory_lag',\n",
        "                                                                 index='explanatory_site')\n",
        "    beta_ar_noweights_wide_df[np.isnan(beta_ar_noweights_wide_df)] = 0\n",
        "\n",
        "    if plot:\n",
        "        white_scale_point = 0 - beta_ar_noweights_df['beta'].min() / (\n",
        "                beta_ar_noweights_df['beta'].max() - beta_ar_noweights_df['beta'].min())\n",
        "        red_white_green_scale = [[0, 'rgba(214, 39, 40, 0.85)'],\n",
        "                                 [white_scale_point, 'rgba(255, 255, 255, 0.85)'],\n",
        "                                 [1, 'rgba(6,54,21, 0.85)']]\n",
        "        px.imshow(beta_ar_noweights_wide_df, color_continuous_scale=red_white_green_scale).show()\n",
        "\n",
        "    return beta_ar_noweights_df, beta_ar_noweights_wide_df, explan_covariance_matrix, xty_matrix, response\n",
        "\n",
        "def buildAR(dataset, horizon, lags, target_site, plot=False):\n",
        "    explan_vars_lagged = buildExplanVar(dataset=dataset, explanatory_sites=[target_site], lags=lags, horizon=horizon,\n",
        "                                        number_plants=1)\n",
        "    explan_vars_lagged_mat = explan_vars_lagged.values\n",
        "    explan_vars_lagged_mat_t = np.transpose(explan_vars_lagged_mat)\n",
        "    explan_covariance_matrix = np.matmul(explan_vars_lagged_mat_t, explan_vars_lagged_mat) / (\n",
        "        explan_vars_lagged.shape[0])\n",
        "    explan_precision_matrix = linalg.inv(explan_covariance_matrix)\n",
        "    xty_matrix = np.matmul(explan_vars_lagged_mat_t, dataset[str(target_site)].values) / \\\n",
        "                 explan_vars_lagged.shape[0]\n",
        "    beta_ar_noweights = np.matmul(explan_precision_matrix, xty_matrix)\n",
        "    beta_ar_noweights_df = pd.DataFrame({'beta': beta_ar_noweights,\n",
        "                                         'explanatory_site': np.append(np.repeat(target_site, len(lags)),\n",
        "                                                                       'intercept'),\n",
        "                                         'explanatory_lag': np.append([lag + horizon for lag in lags], 'NA')})\n",
        "    beta_ar_noweights_wide_df = beta_ar_noweights_df.pivot_table(values='beta', columns='explanatory_lag',\n",
        "                                                                 index='explanatory_site')\n",
        "    # beta_ar_noweights_df.assign(feature=[beta_ar_noweights_df.iloc[i, :]['explanatory_site']+'_lag_'+beta_ar_noweights_df.iloc[i,:]['explanatory_lag'] for i in range(beta_ar_noweights_df.shape[0])])\n",
        "    beta_ar_noweights_wide_df[np.isnan(beta_ar_noweights_wide_df)] = 0\n",
        "\n",
        "    if plot:\n",
        "        white_scale_point = 0 - beta_ar_noweights_df['beta'].min() / (\n",
        "                beta_ar_noweights_df['beta'].max() - beta_ar_noweights_df['beta'].min())\n",
        "        red_white_green_scale = [[0, 'rgba(214, 39, 40, 0.85)'],\n",
        "                                 [white_scale_point, 'rgba(255, 255, 255, 0.85)'],\n",
        "                                 [1, 'rgba(6,54,21, 0.85)']]\n",
        "        px.imshow(beta_ar_noweights_wide_df, color_continuous_scale=red_white_green_scale).show()\n",
        "\n",
        "    return beta_ar_noweights_df, beta_ar_noweights_wide_df\n",
        "\n",
        "\n",
        "def buildARweighted(dataset, weights, conditioning_points, conditioning_bandwidth,\n",
        "                    horizon, lags, target_site, plot=False):\n",
        "    explan_vars_lagged = buildExplanVar(dataset=dataset, explanatory_sites=[target_site], lags=lags, horizon=horizon,\n",
        "                                        number_plants=1)\n",
        "    explan_vars_lagged_mat = explan_vars_lagged.values\n",
        "    explan_vars_lagged_mat_t = np.transpose(explan_vars_lagged_mat)\n",
        "\n",
        "    beta_dfs = []\n",
        "    beta_wide_dfs = []\n",
        "    for i in range(len(weights)):\n",
        "        weights_col = weights[i]\n",
        "        conditioning_point = conditioning_points[i]\n",
        "        explan_weighted_covariance_matrix = np.matmul(explan_vars_lagged_mat_t * weights_col,\n",
        "                                                      explan_vars_lagged_mat) / (explan_vars_lagged.shape[0])\n",
        "        explan_precision_matrix = linalg.inv(explan_weighted_covariance_matrix)\n",
        "        xtwy_matrix = np.matmul(explan_vars_lagged_mat_t * weights_col, dataset[target_site].values) / \\\n",
        "                      explan_vars_lagged.shape[0]\n",
        "        beta = np.matmul(explan_precision_matrix, xtwy_matrix)\n",
        "        beta_df = pd.DataFrame({'beta': beta,\n",
        "                                'conditioning_point': conditioning_point,\n",
        "                                'conditioning_bandwidth': conditioning_bandwidth,\n",
        "                                'explanatory_site': np.append(np.repeat(target_site, len(lags)),\n",
        "                                                              'intercept'),\n",
        "                                'explanatory_lag': np.append([lag + h for lag in lags], 'NA')})\n",
        "        beta_wide_df = beta_df.pivot_table(values='beta', columns='explanatory_lag',\n",
        "                                           index=['explanatory_site', 'conditioning_point', 'conditioning_bandwidth'])\n",
        "        beta_wide_df[np.isnan(beta_wide_df)] = 0\n",
        "        beta_dfs.append(beta_df)\n",
        "        beta_wide_dfs.append(beta_wide_df)\n",
        "    beta_df = pd.concat(beta_dfs)\n",
        "    beta_wide_df = pd.concat(beta_wide_dfs)\n",
        "\n",
        "    if plot:\n",
        "        white_scale_point = 0 - beta_df['beta'].min() / (\n",
        "                beta_df['beta'].max() - beta_df['beta'].min())\n",
        "        red_white_green_scale = [[0, 'rgba(214, 39, 40, 0.85)'],\n",
        "                                 [white_scale_point, 'rgba(255, 255, 255, 0.85)'],\n",
        "                                 [1, 'rgba(6,54,21, 0.85)']]\n",
        "        px.imshow(beta_wide_df, color_continuous_scale=red_white_green_scale).show()\n",
        "\n",
        "    return beta_df, beta_wide_df\n",
        "\n",
        "def buildARSTweighted(dataset, weights, explanatory_sites, conditioning_points, conditioning_bandwidth,\n",
        "                    horizon, lags, target_site, number_plants, plot=False):\n",
        "    explan_vars_lagged = buildExplanVar(dataset=dataset, explanatory_sites=explanatory_sites, lags=lags, horizon=horizon,\n",
        "                                        number_plants=number_plants)\n",
        "    explan_vars_lagged_mat = explan_vars_lagged.values\n",
        "    explan_vars_lagged_mat_t = np.transpose(explan_vars_lagged_mat)\n",
        "\n",
        "    beta_dfs = []\n",
        "    beta_wide_dfs = []\n",
        "    xtx_w_list = []\n",
        "    xty_w_list = []\n",
        "    response_list = []\n",
        "    for i in range(len(weights)):\n",
        "        weights_col = weights[i]\n",
        "        conditioning_point = conditioning_points[i]\n",
        "        explan_weighted_covariance_matrix = np.matmul(explan_vars_lagged_mat_t * weights_col,\n",
        "                                                      explan_vars_lagged_mat)\n",
        "        norm_factor = np.max(explan_weighted_covariance_matrix)\n",
        "        explan_weighted_covariance_matrix = explan_weighted_covariance_matrix / norm_factor\n",
        "        xtx_w_list.append(explan_weighted_covariance_matrix)\n",
        "        explan_precision_matrix = linalg.inv(explan_weighted_covariance_matrix)\n",
        "        xtwy_matrix = np.matmul(explan_vars_lagged_mat_t * weights_col, dataset[str(target_site)].values) / norm_factor\n",
        "        xty_w_list.append(xtwy_matrix)\n",
        "        response_list.append(dataset[str(target_site)].values / norm_factor)\n",
        "\n",
        "        beta = np.matmul(explan_precision_matrix, xtwy_matrix)\n",
        "        beta_df = pd.DataFrame({'beta': beta,\n",
        "                                'conditioning_point': conditioning_point,\n",
        "                                'conditioning_bandwidth': conditioning_bandwidth,\n",
        "                                'explanatory_site': np.append(\n",
        "                                    np.concatenate([np.repeat(site, len(lags)) for site in explanatory_sites]),\n",
        "                                    'intercept'),\n",
        "                                'explanatory_lag': np.append(\n",
        "                                    np.concatenate([[lag + h for lag in lags] for i in range(number_plants)]),'NA')})\n",
        "        beta_wide_df = beta_df.pivot_table(values='beta', columns='explanatory_lag',\n",
        "                                           index=['explanatory_site', 'conditioning_point', 'conditioning_bandwidth'])\n",
        "        beta_wide_df[np.isnan(beta_wide_df)] = 0\n",
        "        beta_dfs.append(beta_df)\n",
        "        beta_wide_dfs.append(beta_wide_df)\n",
        "    beta_df = pd.concat(beta_dfs)\n",
        "    beta_wide_df = pd.concat(beta_wide_dfs)\n",
        "\n",
        "    if plot:\n",
        "        white_scale_point = 0 - beta_df['beta'].min() / (\n",
        "                beta_df['beta'].max() - beta_df['beta'].min())\n",
        "        red_white_green_scale = [[0, 'rgba(214, 39, 40, 0.85)'],\n",
        "                                 [white_scale_point, 'rgba(255, 255, 255, 0.85)'],\n",
        "                                 [1, 'rgba(6,54,21, 0.85)']]\n",
        "        px.imshow(beta_wide_df, color_continuous_scale=red_white_green_scale).show()\n",
        "\n",
        "    return beta_df, beta_wide_df, xtx_w_list, xty_w_list, response_list\n",
        "\n",
        "\n",
        "def buildPrediction(betas, dataset, target_site, explanatory_sites, horizon, lags, number_plants, method_name, **kwargs):\n",
        "    weights = None\n",
        "    plot = None\n",
        "    logit_bool = False\n",
        "    for key, value in kwargs.items():\n",
        "        if key == 'conditioning_bandwidth':\n",
        "            conditioning_bandwidth = value\n",
        "        if key == 'weights':\n",
        "            weights = value\n",
        "        if key == 'conditioning_points':\n",
        "            conditioning_points = value\n",
        "        if key == 'plot':\n",
        "            plot = value\n",
        "        if key == 'logit_bool':\n",
        "            logit_bool = value\n",
        "\n",
        "    explan_vars_lagged_testing = buildExplanVar(dataset=dataset, lags=lags, horizon=horizon,\n",
        "                                                number_plants=number_plants, explanatory_sites=explanatory_sites)\n",
        "\n",
        "    prediction = np.zeros(explan_vars_lagged_testing.shape[0])\n",
        "    if weights is None:\n",
        "\n",
        "        #for column_index in range(explan_vars_lagged_testing.shape[1]):\n",
        "            # print(column_index)\n",
        "        #    feature_index = explan_vars_lagged_testing.columns[column_index]\n",
        "        #    if ' ' in feature_index:\n",
        "        #        explan_site, explan_site_lag = feature_index.split(' ')\n",
        "        #        explan_site_lag = re.findall(r'\\d+', explan_site_lag)[0]\n",
        "\n",
        "        #    else:\n",
        "        #        explan_site = feature_index\n",
        "        #        explan_site_lag = 'NA'\n",
        "        #    beta_column_index = \\\n",
        "        #    betas[(betas['explanatory_site'] == explan_site) & (betas['explanatory_lag'] == explan_site_lag)][\n",
        "        #        'beta'].values[0]\n",
        "        #    prediction = prediction + beta_column_index * explan_vars_lagged_testing[feature_index]\n",
        "\n",
        "        prediction = np.matmul(betas['beta'].values, explan_vars_lagged_testing.T)\n",
        "    else:\n",
        "\n",
        "        sum_weights = weights.sum(axis=1).values # sum of weights for each sample\n",
        "\n",
        "        #for weight_index in range(weights.shape[1]):\n",
        "        #    weight = weights.iloc[:,weight_index]\n",
        "        #    conditioning_point = conditioning_points[weight_index]\n",
        "        #    for column_index in range(explan_vars_lagged_testing.shape[1]):\n",
        "                # print(column_index)\n",
        "        #        feature_index = explan_vars_lagged_testing.columns[column_index]\n",
        "        #        if ' ' in feature_index:\n",
        "        #            explan_site, explan_site_lag = feature_index.split(' ')\n",
        "        #            explan_site_lag = re.findall(r'\\d+', explan_site_lag)[0]\n",
        "        #        else:\n",
        "        #            explan_site = feature_index\n",
        "        #            explan_site_lag = 'NA'\n",
        "        #        beta_column_index = betas[(betas['explanatory_site'] == explan_site) &\n",
        "        #                                  (betas['explanatory_lag'] == explan_site_lag) &\n",
        "        #                                  (betas['conditioning_point'] == conditioning_point)][\n",
        "        #            'beta'].values[0]\n",
        "        #        pred_weight = beta_column_index * weight / sum_weights * explan_vars_lagged_testing[feature_index].values\n",
        "        #        prediction = prediction + pred_weight\n",
        "\n",
        "        for weight_index in range(weights.shape[1]):\n",
        "          weight = weights.iloc[:,weight_index]\n",
        "          conditioning_point = conditioning_points[weight_index]\n",
        "          beta_column_index = betas[betas['conditioning_point'] == conditioning_point]['beta'].values\n",
        "          prediction_weight = np.matmul(beta_column_index, explan_vars_lagged_testing.T)\n",
        "          prediction = prediction + prediction_weight/sum_weights\n",
        "\n",
        "    obs = dataset[str(target_site)]\n",
        "    if logit_bool:\n",
        "        obs = logit_inv(obs)\n",
        "        prediction = logit_inv(prediction)\n",
        "    prediction[prediction < 0] = 0\n",
        "    prediction[prediction > 1] = 1\n",
        "    prediction_df = pd.DataFrame(\n",
        "        {'method': method_name, 'pred': prediction, 'obs': obs.values,\n",
        "         'horizon': horizon, 'datetime': dataset.index.values,\n",
        "         'target_site': str(target_site)})\n",
        "    prediction_df['persistence_h'] = prediction_df['obs'].shift(horizon + 1).fillna(method='bfill')\n",
        "\n",
        "    if plot:\n",
        "        px.line(prediction_df.iloc[:2000,:].melt(id_vars=['datetime', 'horizon', 'method']),\n",
        "            x='datetime', y='value', color='variable').show()\n",
        "\n",
        "    score_tbl = pd.DataFrame({'rmse': [np.sqrt(np.mean((prediction_df['obs'] - prediction_df['pred']) ** 2))],\n",
        "                              'mae': [np.mean(np.abs(prediction_df['obs'] - prediction_df['pred']))],\n",
        "                              'rmse_persist': [\n",
        "                                  np.sqrt(np.mean((prediction_df['obs'] - prediction_df['persistence_h']) ** 2))],\n",
        "                              'mae_persist': [np.mean(np.abs(prediction_df['obs'] - prediction_df['persistence_h']))],\n",
        "                              'horizon': [h],\n",
        "                              'model': method_name})\n",
        "\n",
        "    return score_tbl, prediction_df\n",
        "\n",
        "def builARSTl1Solve_budget(XtX, XtY, l1_threshold, lasso, verbose):\n",
        "    \"\"\"\n",
        "    Deploy and optimize the MIQP formulation of L1-Regression.\n",
        "    \"\"\"\n",
        "\n",
        "    #assert isinstance(l1_threshold, (np.float))\n",
        "    # Create a Gurobi environment and a model object for each conditioning weight\n",
        "\n",
        "    with gp.Env(empty=True) as env:\n",
        "        if not verbose:\n",
        "            env.setParam('OutputFlag', 0)\n",
        "        env.start()\n",
        "        with gp.Model(\"\", env=env) as regressor:\n",
        "            dim = XtY.shape[0]\n",
        "            assert dim == XtX.shape[0]\n",
        "\n",
        "            # Decision variables\n",
        "            norm_1 = regressor.addVar(lb=l1_threshold, ub=l1_threshold, name=\"norm\")\n",
        "            beta = regressor.addMVar((dim,), lb=-GRB.INFINITY, name=\"beta\") # Weights\n",
        "            regressor.setObjective(beta.T @ XtX @ beta - 2 * XtY @ beta, GRB.MINIMIZE)\n",
        "\n",
        "            # Budget constraint based on the L1-norm\n",
        "            if lasso:\n",
        "                regressor.addGenConstrNorm(norm_1, beta[:-1], which=1, name=\"budget\") # respect l1 budget constraint, exclude intercept\n",
        "\n",
        "            #regressor.params.timelimit = 60\n",
        "            regressor.params.mipgap = 0.001\n",
        "            regressor.optimize()\n",
        "\n",
        "            coeff = np.array([beta[i].X for i in range(dim)])\n",
        "\n",
        "        return coeff\n",
        "\n",
        "def builARSTl1Solve_normconstraint(XtX, XtY, response, l1_threshold, verbose=True):\n",
        "    \"\"\"\n",
        "    Deploy and optimize the MIQP formulation of L1-Regression.\n",
        "    ONGOING\n",
        "    \"\"\"\n",
        "\n",
        "    assert isinstance(l1_threshold, (np.float))\n",
        "    # Create a Gurobi environment and a model object for each conditioning weight\n",
        "\n",
        "    with gp.Env(empty=True) as env:\n",
        "        if not verbose:\n",
        "            env.setParam('OutputFlag', 0)\n",
        "        env.start()\n",
        "        with gp.Model(\"\", env=env) as regressor:\n",
        "            dim = XtY.shape[0]\n",
        "            assert dim == XtX.shape[0]\n",
        "\n",
        "            # Decision variables\n",
        "            #norm_1 = regressor.addVar(lb=0, ub=GRB.INFINITY, name=\"norm\")\n",
        "            beta = regressor.addMVar((dim,), lb=-GRB.INFINITY, name=\"beta\") #Weights\n",
        "            regressor.setObjective(beta.T @ XtX @ beta - 2 * XtY @ beta, GRB.MINIMIZE)\n",
        "\n",
        "            # Inequality constraint based on the L1-norm\n",
        "            #regressor.addConstr(norm_1 == gp.norm(beta[:-1], 1.0))\n",
        "            #regressor.addConstr(norm_1 <= l1_threshold)\n",
        "\n",
        "            #regressor.params.timelimit = 60\n",
        "            regressor.params.mipgap = 0.001\n",
        "            regressor.optimize()\n",
        "\n",
        "            coeff = np.array([beta[i].X for i in range(dim)])\n",
        "\n",
        "            return coeff\n",
        "\n",
        "def builARSTl1Solve_normconstraint_vRSS(X, y, l1_threshold, verbose=True):\n",
        "    \"\"\"\n",
        "    Deploy and optimize the MIQP formulation of L1-Regression.\n",
        "    ONGOING\n",
        "    \"\"\"\n",
        "\n",
        "    assert isinstance(l1_threshold, (np.float))\n",
        "    # Create a Gurobi environment and a model object for each conditioning weight\n",
        "\n",
        "    with gp.Env(empty=True) as env:\n",
        "        if not verbose:\n",
        "            env.setParam('OutputFlag', 0)\n",
        "        env.start()\n",
        "        with gp.Model(\"\", env=env) as regressor:\n",
        "\n",
        "            Xval= X.values\n",
        "            yval = y.values\n",
        "            dim = X.shape[1]\n",
        "            samples = X.shape[0]\n",
        "            assert samples == y.shape[0]\n",
        "\n",
        "            # scale features and response\n",
        "\n",
        "            # Decision variables\n",
        "            norm_1 = regressor.addVar(lb=0, ub=GRB.INFINITY, name=\"norm\")\n",
        "            beta = regressor.addMVar((dim,), lb=-GRB.INFINITY, name=\"beta\") # Weights\n",
        "            deviation = regressor.addMVar(samples, lb=-GRB.INFINITY, name='deviation')\n",
        "\n",
        "            regressor.addConstr(deviation == y - Xval @ beta)\n",
        "            loss = deviation @ deviation\n",
        "\n",
        "            # Inequality constraint based on the L1-norm\n",
        "            regressor.addConstr(norm_1 == gp.norm(beta[:-1], 1.0))\n",
        "            regressor.addConstr(norm_1 <= l1_threshold)\n",
        "\n",
        "            # Set objective\n",
        "            regressor.setObjective(loss, GRB.MINIMIZE)\n",
        "\n",
        "            #regressor.params.timelimit = 60\n",
        "            regressor.params.mipgap = 0.001\n",
        "            regressor.optimize()\n",
        "\n",
        "            coeff = np.array([beta[i].X for i in range(dim)])\n",
        "\n",
        "            return coeff\n",
        "\n",
        "\n",
        "def builARSTl1Solve_normineq(XtX, XtY, l1_threshold, lasso=True, verbose=True):\n",
        "    \"\"\"\n",
        "    Deploy and optimize the MIQP formulation of L1-Regression, with pos / neg beta decomposition.\n",
        "    \"\"\"\n",
        "\n",
        "    #assert isinstance(l1_threshold, (np.float))\n",
        "    # Create a Gurobi environment and a model object for each conditioning weight\n",
        "\n",
        "    with gp.Env(empty=True) as env:\n",
        "        if not verbose:\n",
        "            env.setParam('OutputFlag', 0)\n",
        "        env.start()\n",
        "        with gp.Model(\"\", env=env) as regressor:\n",
        "            dim = XtY.shape[0]\n",
        "            assert dim == XtX.shape[0]\n",
        "\n",
        "            # XtX extended to model absolute value\n",
        "            xtx_extended = np.hstack((XtX, np.zeros((dim, dim))))\n",
        "            xtx_extended = np.vstack((xtx_extended, np.zeros((dim, 2 * dim))))\n",
        "            xtx_extended = np.vstack((xtx_extended, np.zeros((1, 2 * dim))))\n",
        "            xtx_extended = np.hstack((xtx_extended, np.zeros((2 * dim + 1, 1))))\n",
        "            xty_extended = np.hstack((XtY, np.zeros((dim + 1))))\n",
        "\n",
        "            # Decision variables\n",
        "            beta_extended = regressor.addMVar((2 * dim + 1,), lb=-GRB.INFINITY, name=\"beta\") # Weights\n",
        "            regressor.setObjective(beta_extended.T @ xtx_extended @ beta_extended - 2 * xty_extended @ beta_extended, GRB.MINIMIZE)\n",
        "\n",
        "            # Expression of absolute norm inequality\n",
        "            regressor.addConstr( -np.ones((1,dim)) @ beta_extended[np.arange(dim, 2 * dim , 1)] - dim * beta_extended[2 * dim] <= -l1_threshold)\n",
        "\n",
        "            for i in range(dim):\n",
        "                regressor.addConstr(- beta_extended[i] + beta_extended[dim + i] + beta_extended[2 * dim] <= 0)\n",
        "                regressor.addConstr(beta_extended[i] + beta_extended[dim + i] + beta_extended[2 * dim] <= 0)\n",
        "\n",
        "            #regressor.params.timelimit = 60\n",
        "            regressor.params.mipgap = 0.001\n",
        "            regressor.optimize()\n",
        "\n",
        "            coeff = np.array([beta_extended[i].X for i in range(dim)])\n",
        "            beta_x = np.array([beta_extended[dim -1 + i].X for i in range(dim)])\n",
        "            beta_s = beta_extended[2 * dim].X\n",
        "        return coeff\n",
        "\n"
      ],
      "metadata": {
        "id": "zLb0SiIqNRT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Auxiliary Functions"
      ],
      "metadata": {
        "id": "pUJCCOq2O2Yq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logit(y):\n",
        "    y[y <= 0] = 0.001\n",
        "    y[y >= 1] = 0.999\n",
        "    x = np.log(y/(1-y))\n",
        "    return x\n",
        "\n",
        "def logit_inv(x):\n",
        "    y = np.exp(-x)/(1+np.exp(-x))\n",
        "    return y\n",
        "\n",
        "def rmse(group):\n",
        "   sites_in_group = group['target_site'].unique()\n",
        "   print(sites_in_group)\n",
        "   rmse_sites = []\n",
        "   for site in sites_in_group:\n",
        "    rez_site = group[group['target_site'] == site]\n",
        "    rmse_site = np.sqrt(np.mean((rez_site['obs'] - rez_site['pred']) ** 2))\n",
        "    rmse_sites.append(rmse_site)\n",
        "   rmse_sites = np.array(rmse_sites)\n",
        "   rmse_std = np.round(np.std(rmse_sites) * 100, decimals=2)\n",
        "   rmse_mean = np.round(np.mean(rmse_sites) * 100, decimals=2)\n",
        "   rmse_allsites = np.round(np.sqrt(np.mean((group['obs'] - group['pred']) ** 2)) * 100, decimals=2)\n",
        "   return rmse_mean, rmse_std, rmse_allsites\n"
      ],
      "metadata": {
        "id": "7nTOKQssO33M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "_afDLDNZNWP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import linalg\n",
        "import re"
      ],
      "metadata": {
        "id": "QsKthwjTZiQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_sites = np.arange(1, 11, 1)#[target_site]\n",
        "\n",
        "# Loop over target sites -----\n",
        "\n",
        "prediction_dfs = []\n",
        "for i in range(0, 1, 1): #len(target_sites), 1):\n",
        "\n",
        "    target_site = target_sites[i]\n",
        "\n",
        "    # A. Loop over testing months\n",
        "    for testing_month_index in np.arange(1, 13, 1):\n",
        "        print('target_site: ' + str(target_site) + ', test month: ' + str(testing_month_index))\n",
        "\n",
        "        wd_train, wd_test_month = train_test_splits_windDirection(dataset_wd, testing_month_index)\n",
        "        prod_train, prod_test_month = train_test_splits_Production(dataset_prod, testing_month_index)\n",
        "\n",
        "        dataset_learning_complete = prod_train.iloc[np.isin(prod_train.index,\n",
        "                                                                          wd_train.index), :]\n",
        "        dataset_testing_complete = prod_test_month.iloc[np.isin(prod_test_month.index,\n",
        "                                                                        wd_test_month.index), :]\n",
        "\n",
        "        wd_train = wd_train.iloc[np.isin(wd_train.index, prod_train.index), :]\n",
        "        wd_test_month = wd_test_month.iloc[np.isin(wd_test_month.index, prod_test_month.index), :]\n",
        "\n",
        "        # B. Loop over conditioning bandwidths\n",
        "        for conditioning_bandwidth in conditioning_winddirection_bandwidths:\n",
        "\n",
        "            #print('start target site ' + str(target_site) + ', bandwidth: ' + str(conditioning_bandwidth))\n",
        "\n",
        "            # ccf_prod_target = ccf_prod.loc[ccf_prod['site_i'] == int(target_site), :]\n",
        "            # px.line(ccf_prod_target, x='lag_value', y='ccf_prod', color='site_j').show()\n",
        "\n",
        "            # Initialize the array of Beta coefficients\n",
        "            betas = np.empty((number_horizons, (number_plants * len(lags) + 1), conditioning_number_points, 1))\n",
        "\n",
        "            # Compute weights for wind direction\n",
        "            weights_train = [\n",
        "                np.exp(-((wd_train[int(target_site)].values - cpoint) ** 2 / (2 * conditioning_bandwidth ** 2)))\n",
        "                for cpoint in conditioning_winddirection_points_positions]\n",
        "            weights_test = [\n",
        "                np.exp(-((wd_test_month[int(target_site)].values - cpoint) ** 2 / (2 * conditioning_bandwidth ** 2)))\n",
        "                for cpoint in conditioning_winddirection_points_positions]\n",
        "            weights_test_df = pd.DataFrame(weights_test).T\n",
        "\n",
        "\n",
        "            cor_prod = cor_prod[np.isin(cor_prod.site_j.astype('string'), dataset_learning_complete.columns)]\n",
        "\n",
        "            # explanatory sites ----\n",
        "            # distance cor proxy\n",
        "            explanatory_sites_distProxy = cor_prod[cor_prod.site_i == int(target_site)].sort_values(by='cor', ascending=False)[:number_plants].site_j.values\n",
        "\n",
        "            # C. Loop over horizons ---\n",
        "            for h in [6]:\n",
        "\n",
        "                #ccf_prod_lags_h = ccf_prod_target.loc[np.abs(ccf_prod_target[\"lag_value\"]) >= h, :]\n",
        "                #ccf_prod_mean_h = ccf_prod_lags_h.groupby('site_j')['ccf_prod'].mean().reset_index().sort_values(\n",
        "                #    by='ccf_prod', ascending=False).iloc[:number_plants, :]\n",
        "                #explanatory_sites_ccfProd = ccf_prod_mean_h['site_j'].values\n",
        "\n",
        "                # 1. analytical solve AR and ARST \\beta = (t(X)X)-1(t(X)Y)\n",
        "\n",
        "                #print('analytical solving AR, for horizon ' +str(h) + '...')\n",
        "                betas_ar, betas_ar_wide = buildAR(dataset=dataset_learning_complete, lags=lags, horizon=h,\n",
        "                                                  target_site=str(target_site), plot=False)\n",
        "\n",
        "\n",
        "                #print('analytical solving ARST, unweighted, for horizon ' + str(h) + '...')\n",
        "                betas_arst_distProxy, betas_arst_wide_distProxy, xtx_distProxy, xty_distProxy, response_distProxy = buildARST(dataset=dataset_learning_complete,\n",
        "                                                        explanatory_sites=explanatory_sites_distProxy,\n",
        "                                                        lags=lags, horizon=h, nb_plants=len(explanatory_sites_distProxy),\n",
        "                                                        target_site=target_site, plot=False)\n",
        "\n",
        "                #print('analytical solving ARST, weighted, for horizon ' + str(h) + '...')\n",
        "                betas_arst_weighted_distProxy, betas_arst_weighted_wide_distProxy, xtx_w_distProxy, xty_w_distProxy, response_w_distProxy= buildARSTweighted(dataset=dataset_learning_complete,\n",
        "                                                                            weights=weights_train,\n",
        "                                                                            conditioning_points=conditioning_winddirection_points_positions,\n",
        "                                                                            conditioning_bandwidth=conditioning_bandwidth,\n",
        "                                                                            explanatory_sites=explanatory_sites_distProxy,\n",
        "                                                                            number_plants=len(explanatory_sites_distProxy),\n",
        "                                                                            horizon=h,\n",
        "                                                                            lags=lags,\n",
        "                                                                            target_site=target_site, plot=False)\n",
        "\n",
        "\n",
        "                #print('evaluate AR...')\n",
        "                ar_scores, pred_ar = buildPrediction(betas=betas_ar, dataset=dataset_testing_complete,\n",
        "                                            target_site = target_site,\n",
        "                                            explanatory_sites=[target_site],\n",
        "                                            horizon=h, lags=lags,\n",
        "                                            number_plants=1, method_name='ar_noweights', plot=False, logit_bool=True)\n",
        "\n",
        "                #print('RMSE AR, month ' + str(testing_month_index) + ': ' + str(ar_scores['rmse'].values))\n",
        "\n",
        "                #print('evaluate ARST - distance proxy...')\n",
        "                arst_distproxy_scores, pred_arst = buildPrediction(betas=betas_arst_distProxy, dataset=dataset_testing_complete,\n",
        "                                                        target_site=target_site, explanatory_sites=explanatory_sites_distProxy,\n",
        "                                                        horizon=h, lags=lags,\n",
        "                                                        number_plants=number_plants, method_name='arst_distProxy_noweights',\n",
        "                                                        logit_bool=True)\n",
        "\n",
        "                #print('RMSE ARST, month ' + str(testing_month_index) + ': '  + str(arst_distproxy_scores['rmse'].values))\n",
        "\n",
        "\n",
        "                arst_weighted_distproxy_scores, pred_arst_weighted = buildPrediction(betas=betas_arst_weighted_distProxy,\n",
        "                                                                dataset=dataset_testing_complete,\n",
        "                                                                target_site=target_site, weights=weights_test_df,\n",
        "                                                                explanatory_sites=explanatory_sites_distProxy,\n",
        "                                                                conditioning_bandwidth=conditioning_bandwidth,\n",
        "                                                                conditioning_points=conditioning_winddirection_points_positions,\n",
        "                                                                horizon=h, lags=lags,\n",
        "                                                                number_plants=number_plants,\n",
        "                                                                method_name='arst_distProxy_weights', logit_bool=True)\n",
        "\n",
        "                #print('RMSE ARST - weighted, month ' + str(testing_month_index) + ': '  + str(arst_distproxy_scores['rmse'].values))\n",
        "\n",
        "\n",
        "                # L1 solve per conditional weight\n",
        "\n",
        "                rmses_list = []\n",
        "                for sparsity_reduc in [2]:\n",
        "\n",
        "                    sparsity_ratio = 1/sparsity_reduc\n",
        "\n",
        "                    features_train = buildExplanVar(dataset=dataset_learning_complete,\n",
        "                                                    lags=lags, horizon=h,\n",
        "                                                    number_plants=number_plants,\n",
        "                                                    explanatory_sites=explanatory_sites_distProxy)\n",
        "\n",
        "                    features_test = buildExplanVar(dataset=dataset_testing_complete,\n",
        "                                                    lags=lags, horizon=h,\n",
        "                                                    number_plants=number_plants,\n",
        "                                                    explanatory_sites=explanatory_sites_distProxy)\n",
        "\n",
        "                    #print('L1 solve for sparsity reduction factor: ' + str(sparsity_reduc)  + '...')\n",
        "\n",
        "                    weighted_lasso = True\n",
        "                    sklearn_lasso = True\n",
        "\n",
        "                    if weighted_lasso:\n",
        "                        #print('solving weighted lasso...')\n",
        "                        beta_list = []\n",
        "                        for cpoint_index in range(len(conditioning_winddirection_points_positions)):\n",
        "\n",
        "                            cpoint = conditioning_winddirection_points_positions[cpoint_index]\n",
        "                            betas_cpoint = betas_arst_weighted_distProxy[betas_arst_weighted_distProxy['conditioning_point'] == cpoint]\n",
        "                            betas_cpoint = betas_cpoint.assign(abs_beta = np.abs(betas_cpoint['beta']))\n",
        "                            beta_intercept = betas_arst_weighted_distProxy[(betas_arst_weighted_distProxy['conditioning_point'] == cpoint) &\n",
        "                                                  (betas_arst_weighted_distProxy['explanatory_site'] == 'intercept')]['beta'].values\n",
        "                            response_cpoint = response_w_distProxy[cpoint_index]\n",
        "\n",
        "                            if sklearn_lasso:\n",
        "                              xtx_w_cpoint = xtx_w_distProxy[cpoint_index]\n",
        "                              xty_w_cpoint = xty_w_distProxy[cpoint_index]\n",
        "\n",
        "                              reg_w = Lasso(alpha=1, random_state=0, fit_intercept=False).fit(\n",
        "                                X=features_train.values,\n",
        "                                y=dataset_learning_complete[str(target_site)].values,\n",
        "                                sample_weight=weights_train[cpoint_index])\n",
        "                              betas_w = reg_w.coef_\n",
        "                            else:\n",
        "                              #xtx_w_cpoint = xtx_w_distProxy[cpoint_index][:-1,:-1]\n",
        "                              #xty_w_cpoint = xty_w_distProxy[cpoint_index][:-1]\n",
        "                              betas_cpoint = betas_cpoint[betas_cpoint['explanatory_site'] != 'intercept']# take out intercept\n",
        "                              betas_sorted_cpoint = betas_cpoint.sort_values(by='abs_beta', ascending=False).reset_index(drop=True)\n",
        "                              betas_threshold_cpoint = betas_sorted_cpoint.iloc[:int(np.floor(betas_sorted_cpoint.shape[0] / sparsity_reduc)),:]\n",
        "\n",
        "                              l1_threshold = np.abs(betas_threshold_cpoint['abs_beta']).sum()\n",
        "\n",
        "                              # derive lasso ----\n",
        "                              betas_w = builARSTl1Solve_normineq(XtX=xtx_w_cpoint, XtY=xty_w_cpoint, lasso=True, l1_threshold=l1_threshold, verbose=False)\n",
        "                              #betas = builARSTl1Solve_normconstraint_vRSS(X=features_train, y=dataset_learning_complete[target_site], l1_threshold=l1_threshold)\n",
        "\n",
        "\n",
        "\n",
        "                            # retrieve results ----\n",
        "                            l1_threshold_optim = np.sum(np.abs(betas_w[:-1]))\n",
        "                            beta_df = pd.DataFrame({'beta': betas_w,\n",
        "                                                    'conditioning_bandwidth': conditioning_bandwidth,\n",
        "                                                    'conditioning_point': cpoint,\n",
        "                                                    'explanatory_site': betas_arst_weighted_distProxy[betas_arst_weighted_distProxy['conditioning_point'] == cpoint]['explanatory_site'],\n",
        "                                                    'explanatory_lag': betas_arst_weighted_distProxy[betas_arst_weighted_distProxy['conditioning_point'] == cpoint]['explanatory_lag'],\n",
        "                                                    'sparsity_ratio': sparsity_ratio})\n",
        "                            beta_list.append(beta_df)\n",
        "                        betas_arst_l1 = pd.concat(beta_list)\n",
        "\n",
        "                        arst_distProxy_w_l1_scores, pred_arst_weighted_l1 = buildPrediction(betas=betas_arst_l1,\n",
        "                                                        dataset=dataset_testing_complete,\n",
        "                                                        target_site=target_site, explanatory_sites=explanatory_sites_distProxy,\n",
        "                                                        conditioning_bandwidth=conditioning_bandwidth,\n",
        "                                                        conditioning_points=conditioning_winddirection_points_positions,\n",
        "                                                        weights=weights_test_df,\n",
        "                                                        horizon=h, lags=lags,\n",
        "                                                        number_plants=number_plants,\n",
        "                                                        method_name='arst_distProxy_w_l1', plot=False, logit_bool=True)\n",
        "\n",
        "                        #print('RMSE ARST cwd lasso sklearn, month ' + str(testing_month_index) + ': ' + str(arst_distProxy_w_l1_scores['rmse'].values))\n",
        "\n",
        "                    betas_arst = betas_arst_distProxy.assign(abs_beta=np.abs(betas_arst_distProxy['beta']))\n",
        "                    betas_arst = betas_arst[betas_arst['explanatory_site'] != 'intercept']\n",
        "                    betas_sorted_arst = betas_arst.sort_values(by='abs_beta', ascending=False).reset_index(drop=True)\n",
        "                    betas_threshold_arst = betas_sorted_arst.iloc[\n",
        "                                            :int(np.floor(betas_sorted_arst.shape[0] / sparsity_reduc)), :]\n",
        "\n",
        "                    l1_threshold = np.abs(betas_threshold_arst['abs_beta']).sum()\n",
        "                    betas = builARSTl1Solve_budget(XtX=xtx_distProxy, XtY=xty_distProxy, lasso=True, l1_threshold=l1_threshold, verbose=False)\n",
        "                    l1_threshold_optim = np.sum(np.abs(betas[:-1]))\n",
        "\n",
        "                    beta_df = pd.DataFrame({'beta': betas,\n",
        "                                            'explanatory_site': betas_arst_distProxy['explanatory_site'],\n",
        "                                            'explanatory_lag': betas_arst_distProxy['explanatory_lag'],\n",
        "                                            'sparsity_ratio': sparsity_ratio})\n",
        "\n",
        "                    arst_distProxy_l1_scores, pred_arst_l1 = buildPrediction(betas=beta_df,\n",
        "                                    dataset=dataset_testing_complete,\n",
        "                                    target_site=target_site, explanatory_sites=explanatory_sites_distProxy,\n",
        "                                    horizon=h, lags=lags,\n",
        "                                    number_plants=number_plants,\n",
        "                                    method_name='arst_distProxy_l1', plot=False, logit_bool=True)\n",
        "\n",
        "                    # build lasso from scikit learn\n",
        "\n",
        "                    #reg = Lasso(alpha=1, random_state=0, fit_intercept=True).fit(X=features_train.iloc[:,:-1].values,\n",
        "                    #                                        y=dataset_learning_complete[str(target_site)].values)\n",
        "                    reg = LassoCV(alphas=alphas_lasso, random_state=0, fit_intercept=True).fit(X=features_train.iloc[:,:-1].values,\n",
        "                                        y=dataset_learning_complete[str(target_site)].values)\n",
        "                    reg_full = LassoCV(alphas=alphas_lasso, random_state=0, fit_intercept=False).fit(X=features_train.values,\n",
        "                                                            y=dataset_learning_complete[str(target_site)].values)\n",
        "                    reg_pred = logit_inv(reg.predict(X=features_test.iloc[:,:-1].values))\n",
        "                    #reg_rmse = np.sqrt(np.mean((logit_inv(dataset_testing_complete[str(target_site)]) - reg_pred)**2))\n",
        "                    #print('RMSE ARST lasso from sklearn, month ' + str(testing_month_index) + ': '  + str(reg_rmse))\n",
        "\n",
        "                    betas_arst_distProxy_l1_sklrn = pd.DataFrame({'beta': np.concatenate((reg.coef_, [reg.intercept_])),\n",
        "                                            'explanatory_site': betas_arst_distProxy['explanatory_site'],\n",
        "                                            'explanatory_lag': betas_arst_distProxy['explanatory_lag'],\n",
        "                                            'sparsity_ratio': sparsity_ratio})\n",
        "\n",
        "                    arst_distProxy_l1_sklrn_scores, pred_arst_l1_sklrn = buildPrediction(betas=betas_arst_distProxy_l1_sklrn,\n",
        "                      dataset=dataset_testing_complete,\n",
        "                      target_site=target_site, explanatory_sites=explanatory_sites_distProxy,\n",
        "                      horizon=h, lags=lags,\n",
        "                      number_plants=number_plants,\n",
        "                      method_name='arst_distProxy_l1_sklrn', plot=False, logit_bool=True)\n",
        "\n",
        "                    pred_arst_l1_sklrn_dir = pred_arst_l1_sklrn.copy()\n",
        "                    pred_arst_l1_sklrn_dir['pred'] = reg_pred\n",
        "                    pred_arst_l1_sklrn_dir['method'] = 'arst_distProxy_l1_sklrn_dir'\n",
        "\n",
        "                    rmses_list.append(arst_distProxy_l1_scores['rmse'])\n",
        "                rmses = pd.concat(rmses_list)\n",
        "\n",
        "                prediction_dfs.append(pred_ar)\n",
        "                prediction_dfs.append(pred_arst)\n",
        "                prediction_dfs.append(pred_arst_l1)\n",
        "                prediction_dfs.append(pred_arst_l1_sklrn)\n",
        "                prediction_dfs.append(pred_arst_l1_sklrn_dir)\n",
        "                prediction_dfs.append(pred_arst_weighted)\n",
        "                prediction_dfs.append(pred_arst_weighted_l1)\n",
        "pred_testing_full = pd.concat(prediction_dfs)\n",
        "print(pred_testing_full.groupby('method').apply(rmse))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mIQ2PqqsNhuy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f4e7424-1ae7-455f-ca2e-e78545c67df5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "target_site: 1, test month: 1\n",
            "target_site: 1, test month: 2\n",
            "target_site: 1, test month: 3\n",
            "target_site: 1, test month: 4\n",
            "target_site: 1, test month: 5\n",
            "target_site: 1, test month: 6\n",
            "target_site: 1, test month: 7\n",
            "target_site: 1, test month: 8\n",
            "target_site: 1, test month: 9\n",
            "target_site: 1, test month: 10\n",
            "target_site: 1, test month: 11\n",
            "target_site: 1, test month: 12\n",
            "['1']\n",
            "['1']\n",
            "['1']\n",
            "['1']\n",
            "['1']\n",
            "['1']\n",
            "['1']\n",
            "method\n",
            "ar_noweights                     (23.0, 0.0, 23.0)\n",
            "arst_distProxy_l1              (22.09, 0.0, 22.09)\n",
            "arst_distProxy_l1_sklrn        (22.08, 0.0, 22.08)\n",
            "arst_distProxy_l1_sklrn_dir    (22.08, 0.0, 22.08)\n",
            "arst_distProxy_noweights       (22.11, 0.0, 22.11)\n",
            "arst_distProxy_w_l1            (26.61, 0.0, 26.61)\n",
            "arst_distProxy_weights         (28.27, 0.0, 28.27)\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "l1_nolasso = np.abs(betas_arst_distProxy['beta'].values).sum()\n",
        "print(np.abs(reg.coef_).sum() / l1_nolasso)\n",
        "print(np.abs(reg_full.coef_).sum() / l1_nolasso)\n",
        "\n",
        "pred_l1_diff = pred_arst_l1_sklrn['pred'] - pred_arst_l1_sklrn_dir['pred']"
      ],
      "metadata": {
        "id": "qcbXBjdaAzBR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aec488fb-c13e-4a91-ffbd-b815806611b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.96\n",
            "0.49349537960968376\n",
            "0.6312243558873352\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_nointercept = logit_inv(np.matmul(np.concatenate((reg.coef_, [reg.intercept_])), features_test.T))\n",
        "\n",
        "\n",
        "pred_comparison_tbl = pd.DataFrame({'pred_dir': pred_arst_l1_sklrn_dir['pred'],\n",
        "                                    'pred_analy':pred_arst_l1_sklrn['pred'],\n",
        "                                    'pred_analy_no_intercept': pred_nointercept})\n",
        "import plotly.express as px\n",
        "px.line(pred_comparison_tbl.reset_index().melt(id_vars='TIMESTAMP'), x='TIMESTAMP', y='value', color='variable').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "I7k7S0I4sgFg",
        "outputId": "cc346c36-007d-4f4d-bda3-c7610c46cc9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-01604f23e95e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred_nointercept\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogit_inv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintercept_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m pred_comparison_tbl = pd.DataFrame({'pred_dir': pred_arst_l1_sklrn_dir['pred'],\n\u001b[1;32m      5\u001b[0m                                     \u001b[0;34m'pred_analy'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpred_arst_l1_sklrn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pred'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'logit_inv' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}